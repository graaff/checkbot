<html> <head>
<title>Checkbot information page</title>
</head>

<body>
<h1>Checkbot</h1>

Checkbot is a tool to verify links on a set of html pages. Checkbot
can check a set of documents on a single server, or on a set of
servers (e.g. all servers within a domain). <p>

Checkbot creates a report which summarizes all links which caused some
kind of error or warning. <p>

Mail about Checkbot can be sent to: <a
href=mailto:checkbot@twi72.twi.tudelft.nl>checkbot@twi72.twi.tudelft.nl</a>.

<h2>How does Checkbot work?</h2>

Checkbot collects all links from URL's matching the <b>match
string</b>, and accessible through the <b>start url</b>. It has two
categories of links: internal links (to other URL's matching the
<b>match string</b>), and external links. The external links are filed
for later use, while all internal links are checked.  <p>

After checking all internal links Checkbot will check all external
links found on the pages. It will <em>always</em> use the
<tt>HEAD</tt> method for this. Checkbot does <em>not</em> adhere to
the robots standard (which involves examining <tt>/robots.txt</tt>
from a site first). However, people have brought some good arguments
to my attention, so I will eventually implement this into Checkbot as
well. <p>

After a interval (which gets progressively longer) Checkbot will write
its current results to a file. This file contains some statistics,
such as number of links processed. It also contains a list of pages
(sorted by server, and per server by page name) which contains links
which generated an HTTP error code, along with that code.<p>

Look at a <a href="sample.html">small sample</a> from Checkbot 1.5 for
my <a href="http://is.twi.tudelft.nl/hci/">HCI Index</a>.

<h2>Usage</h2>

Currently checkbot has the following command-line options:

<table>
  <tr>
    <th>-v</th>
    <td>Verbose mode, print each URL checked, and its result</td>
  </tr>
  <tr>
    <th>-u</th>
    <td>URL to start checking</td>
  </tr>
  <tr>
    <th>-m</th>
    <td>Match. Only URL's matching this string will be considered
	internal URL's.
    </td>
  </tr>
  <tr>
    <th>-x</th>
    <td>Exclude. Exclude URL's which match the string, even when they
	do match the -m match string.
    </td>
</table>

To check a tree of html pages, just start Checkbot with a starting url
and a match argument. Both are mandatory. For example, to check all
"bar" pages on the "foo.org" server, use:<br>
<pre>
    ./checkbot -u http://foo.org/bar/ -m foo.org/bar
</pre>
  
<h2>Availability</h2>

The current release of Checkbot can now be retrieved through the
page. However, note that I consider this to be an
<b><em>ALPHA</em></b> release. I just completed a move from
Perl4/libwww4 to Perl5/libwww5. This has made things quite a bit
cleaner, but I don't really trust the code too much yet. I'll keep
working on it, though. Also, I didn't really pay attention to
portability and configurability yet. <p>

Since this is still ALPHA code, I'd appreciate feedback! Does it work,
what would you like to see, what bugs do you encounter, etc.

<em>Note:</em> Checkbot requires perl 5 and <a
href=http://www.oslonett.no/home/aas/perl/www/>libwww-perl-5b</a>.

Get <a href="checkbot">the latest checkbot release</a>. <p>

Read the <a href="ChangeLog.html">ChangeLog</a> and <a
href=TODO>TODO</a> files for more information.

<h2>Who wrote Checkbot?</h2>

<a href="http://is.twi.tudelft.nl/~graaff/">Hans de Graaff</a> wrote
the last version of Checkbot. Previous versions were written by <a
href="http://www.twi.tudelft.nl/People/D.B.Tischenko.html">Dimitri
Tischenko</a>. Checkbot is originally based on another script by Roy
Fielding.

<hr>
<address></address>
<!-- hhmts start -->
Last modified: Thu Feb 29 08:37:28 MET 1996
<!-- hhmts end -->
</body> </html>
